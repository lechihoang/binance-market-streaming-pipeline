services:
  # Zookeeper - Required for Kafka coordination
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_HEAP_OPTS: "-Xmx256M -Xms128M"
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 300M
        reservations:
          memory: 128M

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_COMPRESSION_TYPE: ${KAFKA_COMPRESSION:-snappy}
      KAFKA_HEAP_OPTS: "-Xmx384M -Xms256M"
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M


  # Redis - Hot cache for real-time data
  redis:
    image: redis:7-alpine
    hostname: redis
    container_name: redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    command: redis-server --maxmemory 64mb --maxmemory-policy allkeys-lru ${REDIS_PASSWORD:+--requirepass $REDIS_PASSWORD}
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 100M
        reservations:
          memory: 64M

  # PostgreSQL - Airflow metadata database
  postgres:
    image: postgres:14-alpine
    hostname: postgres
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${AIRFLOW_POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${AIRFLOW_POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${AIRFLOW_POSTGRES_DB:-airflow}
    ports:
      - "${AIRFLOW_POSTGRES_PORT:-5433}:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${AIRFLOW_POSTGRES_USER:-airflow}"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: postgres -c shared_buffers=64MB -c max_connections=50
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # PostgreSQL - Data storage for warm path (analytics)
  postgres-data:
    image: postgres:14-alpine
    hostname: postgres-data
    container_name: postgres-data
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-crypto}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-crypto}
      POSTGRES_DB: ${POSTGRES_DB:-crypto_data}
    ports:
      - "${POSTGRES_PORT:-5434}:5432"
    volumes:
      - postgres-data-volume:/var/lib/postgresql/data
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER:-crypto}"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: postgres -c shared_buffers=64MB -c max_connections=50
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # MinIO - S3-compatible object storage for cold path (archive)
  minio:
    image: minio/minio:latest
    hostname: minio
    container_name: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    networks:
      - streaming-network
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M


  # Airflow Init - One-time initialization
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    hostname: airflow-init
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USER:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create --username ${AIRFLOW_ADMIN_USER:-admin} --firstname Admin --lastname User --role Admin --email admin@example.com --password ${AIRFLOW_ADMIN_PASSWORD:-admin} || true
        airflow connections add 'spark_default' --conn-type 'spark' --conn-host 'local' --conn-extra '{"queue": "root.default", "deploy-mode": "client"}' || true
        airflow variables set kafka_bootstrap_servers "kafka:29092" || true
        airflow variables set redis_host "redis:${REDIS_PORT:-6379}" || true
        airflow variables set duckdb_path "/opt/airflow/data/streaming.duckdb" || true
        airflow variables set postgres_data_host "postgres-data" || true
        airflow variables set postgres_data_port "5432" || true
        airflow variables set minio_endpoint "minio:9000" || true
        echo "Airflow initialization complete!"
    networks:
      - streaming-network
    restart: on-failure
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M


  # Airflow Webserver
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    hostname: airflow-webserver
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'true'
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/dags/logs
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW__WEBSERVER__RBAC: 'true'
      AIRFLOW__WEBSERVER__WORKERS: '2'
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      REDIS_HOST: redis
      REDIS_PORT: ${REDIS_PORT:-6379}
      DUCKDB_PATH: /opt/airflow/data/streaming.duckdb
      # PostgreSQL Data Storage (Warm Path)
      POSTGRES_HOST: postgres-data
      POSTGRES_PORT: '5432'
      POSTGRES_USER: ${POSTGRES_USER:-crypto}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-crypto}
      POSTGRES_DB: ${POSTGRES_DB:-crypto_data}
      # MinIO Object Storage (Cold Path)
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      MINIO_BUCKET: ${MINIO_BUCKET:-crypto-data}
      MINIO_SECURE: ${MINIO_SECURE:-false}
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
    command: webserver
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 768M
        reservations:
          memory: 384M


  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    hostname: airflow-scheduler
    container_name: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      airflow-webserver:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'true'
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/dags/logs
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      REDIS_HOST: redis
      REDIS_PORT: ${REDIS_PORT:-6379}
      DUCKDB_PATH: /opt/airflow/data/streaming.duckdb
      # PostgreSQL Data Storage (Warm Path)
      POSTGRES_HOST: postgres-data
      POSTGRES_PORT: '5432'
      POSTGRES_USER: ${POSTGRES_USER:-crypto}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-crypto}
      POSTGRES_DB: ${POSTGRES_DB:-crypto_data}
      # MinIO Object Storage (Cold Path)
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      MINIO_BUCKET: ${MINIO_BUCKET:-crypto-data}
      MINIO_SECURE: ${MINIO_SECURE:-false}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
    command: scheduler
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1536M
        reservations:
          memory: 512M


  # Prometheus - Metrics collection and monitoring
  prometheus:
    image: prom/prometheus:latest
    hostname: prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.size=500MB'
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 200M
        reservations:
          memory: 100M

  # Crypto Data API - FastAPI REST API
  crypto-api:
    build:
      context: .
      dockerfile: Dockerfile
    hostname: crypto-api
    container_name: crypto-api
    depends_on:
      redis:
        condition: service_healthy
      postgres-data:
        condition: service_healthy
      minio:
        condition: service_healthy
    ports:
      - "8000:8000"
    environment:
      REDIS_HOST: redis
      REDIS_PORT: ${REDIS_PORT:-6379}
      DUCKDB_PATH: /app/data/streaming.duckdb
      PARQUET_PATH: /app/data/parquet
      # PostgreSQL Data Storage (Warm Path)
      POSTGRES_DATA_HOST: postgres-data
      POSTGRES_DATA_PORT: '5432'
      POSTGRES_DATA_USER: ${POSTGRES_USER:-crypto}
      POSTGRES_DATA_PASSWORD: ${POSTGRES_PASSWORD:-crypto}
      POSTGRES_DATA_DB: ${POSTGRES_DB:-crypto_data}
      # MinIO Object Storage (Cold Path)
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      MINIO_BUCKET: ${MINIO_BUCKET:-crypto-data}
      MINIO_SECURE: ${MINIO_SECURE:-false}
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    command: uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8000/api/v1/system/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 200M
        reservations:
          memory: 100M

  # Grafana - Visualization and monitoring dashboards
  grafana:
    image: grafana/grafana:latest
    hostname: grafana
    container_name: grafana
    depends_on:
      redis:
        condition: service_healthy
      prometheus:
        condition: service_healthy
      crypto-api:
        condition: service_healthy
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GF_SECURITY_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: redis-datasource,marcusolsson-json-datasource
      GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: redis-datasource
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - grafana-storage:/var/lib/grafana
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 300M
        reservations:
          memory: 150M

volumes:
  postgres-db-volume:
  postgres-data-volume:
  minio-data:
  grafana-storage:
  prometheus-data:

networks:
  streaming-network:
    driver: bridge
