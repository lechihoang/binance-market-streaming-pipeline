# PySpark Streaming Processor Configuration Example
# Copy this file to .env.streaming and customize as needed

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_MAX_RATE_PER_PARTITION=100
KAFKA_STARTING_OFFSETS=latest
KAFKA_COMPRESSION_TYPE=snappy

# Kafka Topics
TOPIC_RAW_TRADES=raw_trades
TOPIC_RAW_KLINES=raw_klines
TOPIC_RAW_TICKERS=raw_tickers
TOPIC_PROCESSED_AGGREGATIONS=processed_aggregations
TOPIC_PROCESSED_INDICATORS=processed_indicators
TOPIC_ALERTS=alerts

# Spark Configuration
SPARK_EXECUTOR_MEMORY=250m
SPARK_DRIVER_MEMORY=250m
SPARK_EXECUTOR_CORES=1
SPARK_SHUFFLE_PARTITIONS=2
SPARK_CHECKPOINT_LOCATION=/tmp/spark-checkpoints
SPARK_CHECKPOINT_INTERVAL=30 seconds
SPARK_BACKPRESSURE_ENABLED=true

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_MAX_CONNECTIONS=10
REDIS_SOCKET_TIMEOUT=5
REDIS_SOCKET_CONNECT_TIMEOUT=5
REDIS_CANDLE_TTL_SECONDS=3600
REDIS_INDICATOR_TTL_SECONDS=3600
REDIS_ALERT_LIST_MAX_SIZE=1000

# DuckDB Configuration
DUCKDB_DATABASE_PATH=/tmp/streaming_data.duckdb
DUCKDB_TABLE_CANDLES=candles
DUCKDB_TABLE_INDICATORS=indicators
DUCKDB_TABLE_ALERTS=alerts

# Parquet Configuration
PARQUET_OUTPUT_PATH=/tmp/parquet_output
PARQUET_COMPRESSION=snappy
PARQUET_PARTITION_COLUMNS=date,symbol

# Logging
LOG_LEVEL=INFO
